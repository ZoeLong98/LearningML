{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Title: Speech Filter and Spam Filter\n",
    "\n",
    "Data Description:\n",
    "The bayes_email directory contains two subfolders: ham and spam. The .txt files in the spam folder represent spam emails.\n",
    "\n",
    "**The Naive Bayes Classifier and Its Application**  \n",
    "\n",
    "The Naive Bayes classifier assumes **conditional independence of features**, meaning all attributes are considered independent given a class. Therefore, the class-conditional probability in the original Bayesian formula can be rewritten as the product of individual attribute probabilities:\n",
    "\n",
    "\\[\n",
    "P(x \\mid c) = \\prod_{i=1}^{d} P(x_i \\mid c)\n",
    "\\]\n",
    "\n",
    "In this task, words that appear in the training samples are treated as features for the model. For such discrete features, the conditional probability of the \\(i^{th}\\) attribute belonging to class \\(c\\) is defined as:\n",
    "\n",
    "\\[\n",
    "P(x_i \\mid c) = \\frac{\\text{Number of occurrences of } x_i \\text{ in class } c}{\\text{Total occurrences of all attributes in class } c}\n",
    "\\]\n",
    "\n",
    "When classifying a document, the classifier multiplies the probabilities of individual features to compute the probability of the document belonging to a specific class. However, if any of the probabilities is **zero**, the entire product becomes zero. To mitigate this issue, we initialize all word counts to **1** and the denominator to **2**. This approach is known as **Laplace smoothing**, a widely used technique to handle zero probabilities.\n",
    "\n",
    "Additionally, multiplying many small probabilities can result in **underflow** or numerical precision errors. To address this, the **logarithm** of probabilities is used, converting products into sums, which prevents underflow and improves computational stability:\n",
    "\n",
    "\\[\n",
    "\\log P(x \\mid c) = \\sum_{i=1}^{d} \\log P(x_i \\mid c)\n",
    "\\]"
   ],
   "id": "60e8a517b5523127"
  },
  {
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-10-12T06:47:26.099101Z",
     "start_time": "2024-10-12T06:47:26.091470Z"
    }
   },
   "cell_type": "code",
   "outputs": [],
   "execution_count": 5,
   "source": [
    "import re\n",
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def loadDataSet():\n",
    "    \"\"\"\n",
    "    Function: Create experimental samples\n",
    "    Parameters:\n",
    "        None\n",
    "    Returns:\n",
    "        postingList - Experimental sample split into words\n",
    "        classVec - Category label vector\n",
    "    \"\"\"\n",
    "    postingList=[['my', 'dog', 'has', 'flea', 'problems', 'help', 'please'],\n",
    "                 ['maybe', 'not', 'take', 'him', 'to', 'dog', 'park', 'stupid'],\n",
    "                 ['my', 'dalmation', 'is', 'so', 'cute', 'I', 'love', 'him'],\n",
    "                 ['stop', 'posting', 'stupid', 'worthless', 'garbage'],\n",
    "                 ['mr', 'licks', 'ate', 'my', 'steak', 'how', 'to', 'stop', 'him'],\n",
    "                 ['quit', 'buying', 'worthless', 'dog', 'food', 'stupid']]\n",
    "    classVec = [0,1,0,1,0,1]\n",
    "    return postingList,classVec\n",
    "\n",
    "def createVocabList(dataSet):\n",
    "    \"\"\"\n",
    "    Function: Organize the split experimental sample words into a non-repetitive word list, i.e., vocabulary list\n",
    "    Parameters:\n",
    "        dataSet - Organized sample dataset\n",
    "    Returns:\n",
    "        vocabSet - Returns a non-repetitive word list, i.e., vocabulary list\n",
    "    \"\"\"\n",
    "    vocabSet = []\n",
    "    for sentence in dataSet:\n",
    "        for word in sentence:\n",
    "            if word not in vocabSet:\n",
    "                vocabSet.append(word)\n",
    "\n",
    "    return vocabSet\n",
    "\n",
    "def setOfWords2Vec(vocabList, inputSet):\n",
    "    \"\"\"\n",
    "    Function: Vectorize the inputSet according to the vocabList vocabulary list, each element of the vector is 1 or 0\n",
    "    Parameters:\n",
    "        vocabList - List returned by createVocabList\n",
    "        inputSet - Split word list\n",
    "    Returns:\n",
    "        returnVec - Document vector, word set model\n",
    "    \"\"\"\n",
    "    returnVec = np.zeros(len(vocabList))\n",
    "    for word in inputSet:\n",
    "        if word in vocabList:\n",
    "            returnVec[vocabList.index(word)] += 1\n",
    "\n",
    "    return returnVec.astype(int).tolist()\n",
    "\n",
    "def trainNB(trainMatrix,trainCategory):\n",
    "    \"\"\"\n",
    "    Function: Naive Bayes classifier training function\n",
    "    Parameters:\n",
    "        trainMatrix - Training document matrix, i.e., the matrix composed of returnVec returned by setOfWords2Vec\n",
    "        trainCategory - Training category label vector, i.e., classVec returned by loadDataSet\n",
    "    Returns:\n",
    "        p0Vect - Conditional probability array of non-abusive class\n",
    "        p1Vect - Conditional probability array of abusive class\n",
    "        pAbusive - Probability that the document belongs to the abusive class\n",
    "    \"\"\"\n",
    "    numtrain = len(trainMatrix)\n",
    "    numwords = len(trainMatrix[0])\n",
    "    pAbusive = sum(trainCategory)/float(numtrain)\n",
    "    p0Vect = np.zeros(numwords) + 1\n",
    "    p1Vect = np.zeros(numwords) + 1\n",
    "    p0Denom = 2\n",
    "    p1Denom = 2\n",
    "    for i in range(numtrain):\n",
    "        if trainCategory[i] == 1:\n",
    "            p1Vect += trainMatrix[i]\n",
    "            p1Denom += sum(trainMatrix[i])\n",
    "        else:\n",
    "            p0Vect += trainMatrix[i]\n",
    "            p0Denom += sum(trainMatrix[i])\n",
    "    p1Vect = p1Vect/p1Denom\n",
    "    p0Vect = p0Vect/p0Denom\n",
    "\n",
    "    return p0Vect, p1Vect, pAbusive\n",
    "\n",
    "def classifyNB(vec2Classify, p0Vec, p1Vec, pClass1):\n",
    "    \"\"\"\n",
    "    Function: Naive Bayes classifier classification function\n",
    "    Parameters:\n",
    "        vec2Classify - Word array to be classified\n",
    "        p0Vec - Conditional probability array of non-abusive class\n",
    "        p1Vec - Conditional probability array of abusive class\n",
    "        pClass1 - Probability that the document belongs to the abusive class\n",
    "    Returns:\n",
    "        0 - Belongs to non-abusive class\n",
    "        1 - Belongs to abusive class\n",
    "    \"\"\"\n",
    "    p1 = 1; p0 = 1;\n",
    "    for i in range(len(vec2Classify)):\n",
    "        if vec2Classify[i] != 0:\n",
    "            p1 *= p1Vec[i]\n",
    "            p0 *= p0Vec[i]\n",
    "    p1 = np.log(p1*pClass1)\n",
    "    p0 = np.log(p0*(1.0-pClass1))\n",
    "    if p1 > p0:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "def testingNB():\n",
    "    \"\"\"\n",
    "    Function: Test the Naive Bayes classifier\n",
    "    Test sample 1: ['love', 'my', 'dalmation']\n",
    "    Test sample 2: ['stupid', 'garbage']\n",
    "    Parameters:\n",
    "        None\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    postingList, classVec = loadDataSet()\n",
    "    vocabSet = createVocabList(postingList)\n",
    "    vec_train = []\n",
    "    for sentence in postingList:\n",
    "        vec_train.append(setOfWords2Vec(vocabSet, sentence))\n",
    "    p0Vect, p1Vect, pAbusive = trainNB(vec_train, classVec)\n",
    "\n",
    "    test = [['love', 'my', 'dalmation'], ['stupid', 'garbage']]\n",
    "    vec_test = []\n",
    "    for sentence in test:\n",
    "        vec_test.append(setOfWords2Vec(vocabSet, sentence))\n",
    "    for i in range(len(vec_test)):\n",
    "        result = classifyNB(vec_test[i], p0Vect, p1Vect, pAbusive)\n",
    "        if result == 1:\n",
    "            print('{} belongs to abusive class'.format(test[i]))\n",
    "        else:\n",
    "            print('{} belongs to non-abusive class'.format(test[i]))\n",
    "\n",
    "def textParse(bigString):\n",
    "    \"\"\"\n",
    "    Function: Receive a string and parse it into a list of words\n",
    "    Parameters:\n",
    "        bigString - String\n",
    "    Returns:\n",
    "        List of words (except for single letters, such as uppercase I, other words are converted to lowercase, and strings with a length of less than 3 are filtered)\n",
    "    \"\"\"\n",
    "    bigString = re.sub(r'[\\W_]+', ' ', bigString)\n",
    "    raw = bigString.split()\n",
    "    words = []\n",
    "    for word in raw:\n",
    "        if len(word) >= 3:\n",
    "            words.append(word.lower())\n",
    "\n",
    "    return words\n",
    "\n",
    "def spamTest():\n",
    "    \"\"\"\n",
    "    Function: Divide the dataset into training and test sets, and use cross-validation to test the accuracy of the Naive Bayes classifier\n",
    "    \"\"\"\n",
    "    # Get text data and labels\n",
    "    filenames = []\n",
    "    path_pos = 'datasets//bayes_email//ham'\n",
    "    path_neg = 'datasets//bayes_email//spam'\n",
    "    for files in os.listdir(path_pos):\n",
    "        if files.endswith('txt'):\n",
    "            file = os.path.join(path_pos, files)\n",
    "            filenames.append(file)\n",
    "    for files in os.listdir(path_neg):\n",
    "        if files.endswith('txt'):\n",
    "            file = os.path.join(path_neg, files)\n",
    "            filenames.append(file)\n",
    "    data = []\n",
    "    classlist = []\n",
    "    for filename in filenames:\n",
    "        with open(filename, encoding='cp1252') as file:\n",
    "            data.append(textParse(file.read()))\n",
    "        if 'ham' in filename:\n",
    "            classlist.append(0)\n",
    "        else:\n",
    "            classlist.append(1)\n",
    "\n",
    "    # Get vocabulary list and convert to vector\n",
    "    vocablist = createVocabList(data)\n",
    "    datamat = []\n",
    "    for da in data:\n",
    "        datamat.append(setOfWords2Vec(vocablist, da))\n",
    "\n",
    "    # Divide training and test sets\n",
    "    index = random.sample(range(50), 50)\n",
    "    trainset = []; trainclass = [];\n",
    "    testset = []; testclass = [];\n",
    "    for i in range(0,40):\n",
    "        trainset.append(datamat[index[i]])\n",
    "        trainclass.append(classlist[index[i]])\n",
    "    for i in range(40, 50):\n",
    "        testset.append(datamat[index[i]])\n",
    "        testclass.append(classlist[index[i]])\n",
    "\n",
    "    # Start training and testing\n",
    "    p0V, p1V, pSpam = trainNB(trainset, trainclass)\n",
    "    errorcount = 0\n",
    "    for i in range(len(testset)):\n",
    "        result = classifyNB(testset[i], p0V, p1V, pSpam)\n",
    "        if result != testclass[i]:\n",
    "            errorcount += 1\n",
    "            print('{} was misclassified, the classification result is {}, the true result is {}'.format(data[index[40+i]], result, testclass[i]))\n",
    "\n",
    "    print('Error rate: {}'.format(errorcount/len(testset)))"
   ],
   "id": "initial_id"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-12T06:47:26.116687Z",
     "start_time": "2024-10-12T06:47:26.103683Z"
    }
   },
   "cell_type": "code",
   "source": [
    "testingNB()\n",
    "spamTest()"
   ],
   "id": "4ec842bfb032950d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['love', 'my', 'dalmation'] belongs to non-abusive class\n",
      "['stupid', 'garbage'] belongs to abusive class\n",
      "['home', 'based', 'business', 'opportunity', 'knocking', 'your', 'door', 'don', 'rude', 'and', 'let', 'this', 'chance', 'you', 'can', 'earn', 'great', 'income', 'and', 'find', 'your', 'financial', 'life', 'transformed', 'learn', 'more', 'here', 'your', 'success', 'work', 'from', 'home', 'finder', 'experts'] was misclassified, the classification result is 0, the true result is 1\n",
      "Error rate: 0.1\n"
     ]
    }
   ],
   "execution_count": 6
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
